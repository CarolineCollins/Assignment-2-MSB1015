---
title: "pls vignette"
author: "Caroline Collins 6192527"
date: "26 September 2019"
output: html_document
---
Following the vignette at
https://cran.r-project.org/web/packages/pls/vignettes/pls-manual.pdf
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
There are some example datasets loaded in the pls package.
yarn 
A data set with 28 near-infrared spectra (NIR) of PET yarns, measured at 268 wavelengths, as predictors, and density as response (density) [20]. 
The data set also includes
a logical variable train which can be used to split the data into a training data set of
size 21 and test data set of size 7. See ?yarn for details.
oliveoil
A data set with 5 quality measurements (chemical) and 6 panel sensory panel
variables (sensory) made on 16 olive oil samples [15]. See ?oliveoil for details.
gasoline 
A data set consisting of octane number (octane) and NIR spectra (NIR) of 60 gasoline samples [10]. Each NIR spectrum consists of 401 diffuse reflectance measurements
from 900 to 1700 nm. See ?gasoline for details.

```{r}
library(pls)
data(yarn)
data(oliveoil)
data(gasoline)

```
We first divide the data set into train and test data sets:

```{r}
gasTrain <- gasoline[1:50,]
gasTest <- gasoline[51:60,]
```
A typical way of fitting a PLSR model is
```{r}
gas1 <- plsr(octane ~ NIR, ncomp = 10, data = gasTrain, validation = "LOO")
```
This fits a model with 10 components, 
and includes leave-one-out (LOO) cross-validated predictions. 
We can get an overview of the fit and validation results with the summary method:
```{r}
summary(gas1)
```
The validation results here are Root Mean Squared Error of Prediction (RMSEP). 
There are 2 cross-validation estimates: CV is the ordinary CV estimate, and adjCV is a bias-corrected CV estimate
[ref: Bjørn-Helge Mevik and Henrik Ren´e Cederkvist. Mean squared error of prediction
(MSEP) estimates for principal component regression (PCR) and partial least squares
regression (PLSR). Journal of Chemometrics, 18(9):422-429, 2004.]. 
For a LOO CV, there is virtually no difference between adjCV and CV.
It is often simpler to judge the RMSEPs by plotting them:
```{r}
plot(RMSEP(gas1), legendpos = "topright")

```
This "elbow plot" plots the estimated RMSEPs as functions of the number of components (Figure 2).
The legendpos argument adds a legend at the indicated position. 
Two components seem to be enough to give a good RMSEP of 0.297. 
As mentioned in the introduction, the main practical difference between PCR and PLSR is that PCR often needs more components than PLSR to achieve the same prediction error. 
On this data set, PCR would need three components to achieve the same RMSEP.
====

Once the number of components has been chosen, one can inspect different aspects of the
fit by plotting predictions, scores, loadings, etc. 

The default plot is a prediction plot:
```{r}
plot(gas1, ncomp = 2, asp = 1, line = TRUE)
```
This shows the cross-validated predictions with two components versus measured values, an aspect ratio of 1, and a target line. 
The points follow
the target line and there is no indication of a curvature or other anomalies.
=====

Other plottypes can be selected with the plotttype argument. The following code gives a pairwise plot of the score values for the three first components.
Score plots are often used to look for patterns, groups or outliers in the data. (For instance,plotting the two first components for a model built on the yarn dataset clearly indicates theexperimental design of that data.) In this example, there is no clear indication of grouping or outliers. The numbers in parentheses after the component labels are the relative amount of X variance explained by each component. 


```{r}
plot(gas1, plottype = "scores", comps = 1:3)
```
The explained variances can be extracted explicitly with
```{r}
explvar(gas1)

```
The loading plot is much used for interpretation purposes, for instance to look
for known spectral peaks or profiles. The labels = "numbers" argument makes the plot function try to interpret the variable names as numbers, and use them as x axis labels.
```{r}
plot(gas1, "loadings", comps = 1:2, legendpos = "topleft",  labels = "numbers", xlab = "nm")
abline(h = 0)
```

A fitted model is often used to predict the response values of new observations. 
The following predicts responses for the 10 observations in gasTest, using 2 components:
```{r}
predict(gas1, ncomp = 2, newdata = gasTest)

```
Because we know the true response values for these samples, we can calculate the test set
RMSEP:
```{r}
RMSEP(gas1, newdata = gasTest)
```
=============================
##Formulas and data frames

The pls package has a formula interface that works like the formula interface in R's standard lm functions, in most ways. 
This section gives a short description of formulas and dataframes as they apply to pls. More information on formulas can be found in the lm help file,
in Chapter 11 of 'An Introduction to R', and in Chapter 2 of 'The White Book' [ref:John M. Chambers and Trevor J. Hastie. Statistical Models in S. Chapman & Hall,
London, 1992.]. 

Formulas
=======
A formula consists of a left hand side (lhs), a tilde (~), and a right hand side (rhs).
The lhs consists of a single term, representing the response(s). 
The rhs consists of one or more terms separated by +, representing the regressor(s).
The intercept is handled automatically and doesn't need to be specified.
The terms might be a matrix, a numeric vector or a factor (a factor should not be used
as the response).
If the response term is a matrix, a multi-response model is fit.
In pls, the RHS quite often consists of a single term, representing a matrix regressor: 
y ~ X.
It is also possible to specify transformations of the variables. 
For instance, log(y) ~ msc(Z) 
If the transformations contain symbols that are interpreted in the
formula handling, e.g., +, * or ^, then the terms should be protected with the I() function, like this: y ~ x1 + I(x2 + x3). 
This specifies two regressors: x1, and the sum of x2 and x3.

Dataframes
=========
The fit functions first look for the specified variables in a supplied data frame, 
and it is advisable to collect all variables there. 
This makes it easier to know what data has been used
for fitting, to keep different variants of the data around, and to predict new data.
To create a dataframe, one can use the data.frame function: 
if v1, v2 and v3 are factors
or numeric vectors, 
mydata <- data.frame(y = v1, a = v2, b = v3) will result in a data
frame with variables named y, a and b.
multi-response models require a matrix as the
response term. If Z is a matrix, it has to be protected by the 'protect function' I() in calls
to data.frame: mydata <- data.frame(..., Z = I(Z)). Otherwise, it will be split into
separate variables for each column, and there will be no variable called Z in the data frame,
so we cannot use Z in the formula. One can also add the matrix to an existing data frame:
eg mydata <- data.frame(...)
   mydata$Z <- Z
This will also prevent Z from being split into separate variables. Finally, one can use cbind
to combine vectors and matrices into matrices on the fly in the formula. This is most useful for the response, 
e.g., cbind(y1, y2) ~ X.
Variables in a dataframe can be accessed with the $ operator, e.g., mydata$y. However,
the pls functions access the variables automatically, so the user should never use $ in formulas.

##Fitting models
The main functions for fitting models are pcr and plsr. (They are simply wrappers for the
function mvr, selecting the appropriate fit algorithm). We will use plsr in the examples in
this section, but everything could have been done with pcr (or mvr).
In its simplest form, the function call for fitting models is plsr(formula, ncomp, data)
(where plsr can be substituted with pcr or mvr). The argument formula is a formula as
described above, ncomp is the number of components one wishes to fit, and data is the data
frame containing the variables to use in the model. The function returns a fitted model
(an object of class "mvr") which can be inspected (Section 7) or used for predicting new
observations (Section 8). For instance:
```{r}
 dens1 <- plsr(density ~ NIR, ncomp = 5, data = yarn)

```

If the response term of the formula is a matrix, a multi-response model is fit, e.g.,
```{r}
dim(oliveoil$sensory)

```

```{r}
plsr(sensory ~ chemical, data = oliveoil)
```

The argument ncomp is optional. If it is missing, the maximal possible number of components are used. Also data is optional, and if it is missing, the variables specified in the formula is searched for in the global environment (the user's workspace). Usually, it is preferable to
keep the variables in data frames, but it can sometimes be convenient to have them in the
global environment. If the variables reside in a data frame, 
e.g. yarn, do not be tempted to
use formulas like yarn$density ~ yarn$NIR! Use density ~ NIR and specify the data frame
with data = yarn as above.
There are facilities for working interactively with models. To use only part of the samples
in a data set, for instance the first 20, one can use arguments subset = 1:20 or data =
yarn[1:20,]. Also, if one wants to try different alternatives of the model, one can use the
function update. For instance
```{r}
trainind <- which(yarn$train == TRUE)
dens2 <- update(dens1, subset = trainind)
dens3 <- update(dens1, ncomp = 10)

```
dens2 is a refit of the model dens1 using only the observations which are marked as TRUE in yarn$train,
and dens3 is the result of changing the number of components to 10.
Other arguments, such as formula, can also be
changed with update. This can save a bit of typing when working interactively with models
(but it doesn't save computing time; the model is refitted each time).
===
Missing data can sometimes be a problem. The PLSR and PCR algorithms currently
implemented in pls do not handle missing values intrinsically, so observations with missing
values must be removed. This can be done with the na.action argument. With na.action
= na.omit (the default), any observation with missing values will be removed from the model
completely. With na.action = na.exclude, they will be removed from the fitting process,
but included as NAs in the residuals and fitted values. If you want an explicit error when there
are missing values in the data, use na.action = na.fail. The default na.action can be set
with options(), e.g., options(na.action = quote(na.fail)).
Standardisation and other pre-treatments of predictor variables are often called for. In
pls, the predictor variables are always centered, as a part of the fit algorithm. Scaling can
be requested with the scale argument. If scale is TRUE, each variable is standardised by
dividing it by its standard deviation, and if scale is a numeric vector, each variable is divided
by the corresponding number. For instance, this will fit a model with standardised chemical
measurements:
```{r}
olive1 <- plsr(sensory ~ chemical, scale = TRUE, data = oliveoil)
```

There are other arguments that can be given in the fit call: validation is for selecting
validation, and ... is for sending arguments to the underlying functions, notably the crossvalidation function mvrCv. For the other arguments, see ?mvr.

##Choosing the number of components with cross-validation
Cross-validation, commonly used to determine the optimal number of components to take
into account, is controlled by the validation argument in the modelling functions (mvr,
plsr and pcr). The default value is "none". Supplying a value of "CV" or "LOO" will cause
the modelling procedure to call mvrCv to perform cross-validation; "LOO" provides leaveone-out cross-validation, whereas "CV" divides the data into segments. Default is to use ten
segments, randomly selected, but also segments of consecutive objects or interleaved segments
(sometimes also referred to as 'Venetian blinds') are possible through the use of the argument
segment.type. One can also specify the segments explicitly with the argument segments;
see ?mvrCv for details.
When validation is performed in this way, the model will contain an element comprising
information on the out-of-bag predictions (in the form of predicted values, as well as MSEP
and R2 values). As a reference, the MSEP error using no components at all is calculated as
well. The validation results can be visualised using the plottype = "validation" argument
of the standard plotting function. 
typically, one would select a number of components after which the cross-validation error does
not show a significant decrease.
The decision on how many components to retain will to some extent always be subjective.
However, especially when building large numbers of models (e.g., in simulation studies), it can
be crucial to have a consistent strategy on how to choose the "optimal" number of components.
Two such strategies have been implemented in function selectNcomp. 
1. The first is based on the
so-called one-sigma heuristic and consists of choosing the model with fewest components
that is still less than one standard error away from the overall best model.
2. The second strategy
employs a permutation approach, and basically tests whether adding a new component is
beneficial at all [21]. It is implemented backwards, again taking the global minimum in
the crossvalidation curve as a starting point, and assessing models with fewer and fewer
components: as long as no significant deterioration in performance is found (by default on
the ?? = 0.01 level), the algorithm continues to remove components. Applying the function is
quite straightforward:
```{r}
gas2 <- plsr(octane ~ msc(NIR), ncomp = 10, data = gasTrain)
predict(gas2, ncomp = 3, newdata = gasTest)
ncomp.onesigma <- selectNcomp(gas2, method = "onesigma", plot = TRUE, ylim = c(.18, .6))
ncomp.permut <- selectNcomp(gas2, method = "randomization", plot = TRUE, ylim = c(.18, .6))
```
```{r}
gas2.cv <- crossval(gas2, segments = 10)

summary(gas2.cv, what = "validation")

```
```{r}
plot(MSEP(gas2.cv), legendpos="topright")
```

##7 Inspecting fitted models
A closer look at the fitted model may reveal interesting agreements or disagreements with
what is known about the relations between X and Y. Several functions are implemented in
pls for plotting, extracting and summarising model components.
The regression coefficients can be visualised using plottype = "coef" in the plot method,
or directly through function coefplot. This allows simultaneous plotting of the regression
vectors for several different numbers of components at once. The regression vectors for the
gasoline data set using MSC
```{r}
plot(gas1, plottype = "coef", ncomp=1:3, legendpos = "bottomleft",labels = "numbers", xlab = "nm")

```
Note that the coefficients for two components and three components are similar. This is
because the third component contributes little to the predictions. The RMSEPs 
and predictions  for two and three components are quite similar.
Scores and loadings can be plotted using functions scoreplot and loadingplot, respectively. One can indicate the number of
components with the comps argument; if more than two components are given, plotting the
scores will give a pairs plot, otherwise a scatter plot. For loadingplot, the default is to use
line plots.
Finally, a 'correlation loadings' plot (function corrplot, or plottype = "correlation"
in plot) 
shows the correlations between each variable and the selected components. These plots are scatter plots of two sets of scores with concentric circles of radii given
by radii. Each point corresponds to an X variable. The squared distance between the point
and the origin equals the fraction of the variance of the variable explained by the components
in the panel. The default values for radii correspond to 50% and 100% explained variance,
respectively
The plot functions accept most of the ordinary plot parameters, such as col and pch. If
the model has several responses or one selects more than one model size, e.g. ncomp = 4:6,
in some plot functions (notably prediction plots (see below), validation plots and coefficient
plots) the plot window will be divided and one plot will be shown for each combination of
response and model size. The number of rows and columns are chosen automatically, but can
be specified explicitly with arguments nRows and nCols. If there are more plots than fit the
plot window, one will be asked to press return to see the rest of the plots.
##7.2 Extraction
Regression coefficients can be extracted using the generic function coef; the function takes
several arguments, indicating the number of components to take into account, and whether
the intercept is needed (default is FALSE).
Scores and loadings can be extracted using functions scores and loadings for X, and
Yscores and Yloadings for Y. These also return the percentage of variance explained as
attributes. In PLSR, weights can be extracted using the function loading.weights. When
applied to a PCR model, the function returns NULL.
Note that commands like plot(scores(gas1)) are perfectly correct, and lead to exactly
the same plots as using scoreplot.
##7.3 Summaries
The print method for an object of class "mvr" shows the regression type used, perhaps
indicating the form of validation employed, and shows the function call. The summary method
gives more information: it also shows the amount of variance explained by the model (for all
choices of a, the number of latent variables). The summary method has an additional argument
(what) to be able to focus on the training phase or validation phase, respectively. Default is
to print both types of information.
```{r}
plot(gas1, plottype = "correlation", ncomp=1:3, legendpos = "bottomleft",labels = "numbers", xlab = "nm")
```
##8 Predicting new observations
Fitted models are often used to predict future observations, and pls implements a predict
method for PLSR and PCR models. The most common way of calling this function is
predict(mymod, ncomp = myncomp, newdata = mynewdata), where mymod is a fitted model,
myncomp specifies the model size(s) to use, and mynewdata is a data frame with new X observations. The data frame can also contain response measurements for the new observations,
which can be used to compare the predicted values to the measured ones, or to estimate the
overall prediction ability of the model. If newdata is missing, predict uses the data used to
fit the model, i.e., it returns fitted values.
If the argument ncomp is missing, predict returns predictions for models with 1 component, 2 components, . . ., A components, where A is the number of components used when
fitting the model. Otherwise, the model size(s) listed in ncomp are used. For instance, to get
predictions from the model built in Section 3, with two and three components, one would use

```{r}
predict(gas1, ncomp = 2:3, newdata = gasTest[1:5,])

```
(We predict only the five first test observations, to save space.) The predictions with two and
three components are quite similar. This could be expected, given that the regression vectors
(Figure 7) as well as the estimated RMSEPs for the two model sizes were similar.
One can also specify explicitly which components to use when predicting. This is done by
specifying the components in the argument comps. (If both ncomp and comps are specified,
comps takes precedence over ncomp.) For instance, to get predictions from a model with only
component 2, one can use
```{r}
predict(gas1, comps = 2, newdata = gasTest[1:5,])

```

The results are different from the predictions with two components (i.e., components one
and two) above. (The intercept is always included in the predictions. It can be removed by
subtracting mymod$Ymeans from the predicted values.)
The predict method returns a three-dimensional array, in which the entry (i, j, k) is
the predicted value for observation i, response j and model size k. Note that singleton
dimensions are not dropped, so predicting five observations for a uni-response model with
ncomp = 3 gives an 5 × 1 × 1 array, not a vector of length five. This is to make it easier
to distinguish between predictions from models with one response and predictions with one
model size. (When using the comps argument, the last dimension is dropped, because the
predictions are always from a single model.) One can drop the singleton dimensions explicitly
by using drop(predict(...)):

```{r}
drop(predict(gas1, ncomp = 2:3, newdata = gasTest[1:5,]))

```
##missing values
Missing values in newdata are propagated to NAs in the predicted response, by default.
This can be changed with the na.action argument. See ?na.omit for details.
The newdata does not have to be a data frame. Recognising the fact that the right hand
side of PLSR and PCR formulas very often are a single matrix term, the predict method
allows one to use a matrix as newdata, so instead of
```{r}
# newdataframe <- data.frame(X = newmatrix)
# predict(..., newdata = newdataframe)

```
you can use
```{r}
# predict(..., newdata = newmatrix)
```

However, there are a couple of caveats: First, this only works in predict. Other functions
that take a newdata argument (such as RMSEP) must have a data frame (because they also
need the response values). Second, when newdata is a data frame, predict is able to perform
more tests on the supplied data, such as the dimensions and types of variables. Third,
with the exception of scaling (specified with the scale argument when fitting the model),
any transformations or coding of factors and interactions have to be performed manually if
newdata is a matrix.
It is often interesting to predict scores from new observations, instead of response values.
This can be done by specifying the argument type = "scores" in predict. One will then
get a matrix with the scores corresponding to the components specified in comps (ncomp is
accepted as a synonym for comps when predicting scores).
Predictions can be plotted with the function predplot. This function is generic, and
can also be used for plotting
```{r}
predplot(gas1, ncomp = 2, newdata = gasTest, asp = 1, line = TRUE)

```

This plots predicted (with 2 components) versus measured response values. (Note that
newdata must be a data frame with both X and Y variables.)
##9.3 Package design
The pls package is designed such that an interface function mvr handles the formula and
data, and calls an underlying fit function (and possibly a cross-validation function) to do the
real work. There are several reasons for this design: it makes it easier to implement new
algorithms, one can easily skip the time-consuming formula and data handling in computingintensive applications (simulations, etc.), and it makes it easier to use the pls package as a
building block in other packages.
The plotting facilities are implemented similarly: the plot method simply calls the correct
plot function based on the plottype argument. Here, however, the separate plot functions are
meant to be callable interactively, because some people like to use the generic plot function,
while others like to use separate functions for each plot type. There are also plot methods
for some of the components of fitted models that can be extracted with extract functions, like
score and loading matrices. Thus there are several ways to get some plots, e.g.:

```{r}
plot(mymod, plottype = "scores", ...)
scoreplot(mymod, ...)
plot(scores(mymod), ...)
```


One example of a package that uses pls is lspls, available on CRAN. In that package LS is
combined with PLS in a regression procedure. It calls the fit functions of pls directly, and also
uses the plot functions to construct score and loading plots. There is also the plsgenomics
package, which includes a modified version of (an earlier version of) the SIMPLS fit function
simpls.fit.

##9.4 Calling fit functions directly
The underlying fit functions are called kernelpls.fit, oscorespls.fit, and simpls.fit
for the PLSR methods, and svdpc.fit for the PCR method. They all take arguments X, Y,
ncomp, and stripped. Arguments X, Y, and ncomp specify X and Y (as matrices, not data
frames), and the number of components to fit, respectively. The argument stripped defaults
to FALSE. When it is TRUE, the calculations are stripped down to the bare minimum required
for returning the X means, Y means, and the regression coefficients. This is used to speed
up cross-validation procedures.
The fit functions can be called directly, for instance when one wants to avoid the overhead
of formula and data handling in repeated fits. As an example, this is how a simple leave-oneout cross-validation for a uni-response-model could be implemented, using the SIMPLS:
```{r}
X <- gasTrain$NIR
Y <- gasTrain$octane
ncomp <- 5
cvPreds <- matrix(nrow = nrow(X), ncol = ncomp)
for (i in 1:nrow(X)) {
fit <- simpls.fit(X[-i,], Y[-i], ncomp = ncomp, stripped = TRUE)
cvPreds[i,] <- (X[i,] - fit$Xmeans) %*% drop(fit$coefficients) +
fit$Ymeans
}
```

The RMSEP of the cross-validated predictions are

```{r}
sqrt(colMeans((cvPreds - Y)^2))

```

which can be seen to be the same as the (unadjusted) CV results for the gas1 model in
Section 3.
##9.5 Formula handling in more detail
The handling of formulas and variables in the model fitting is very similar to what happens in
the function lm: The variables specified in the formula are looked up in the data frame given
in the data argument of the fit function (plsr, pcr or mvr), or in the calling environment
if not found in the data frame. Factors are coded into one or more of columns, depending
on the number of levels, and on the contrasts option. All (possibly coded) variables are then
collected in a numerical model matrix. This matrix is then handed to the underlying fit or
cross-validation functions. A similar handling is used in the predict method.
The intercept is treated specially in pls. After the model matrix has been constructed,
the intercept column is removed. This ensures that any factors are coded as if the intercept
was present. The underlying fit functions then center the rest of the variables as a part of the
fitting process. (This is intrinsic to the PLSR and PCR algorithms.) The intercept is handled
separately. A consequence of this is that explicitly specifying formulas without the intercept
(e.g., y ~ a + b - 1) will only result in the coding of any factors to change; the intercept
will still be fitted.
